Recall that we introduced matrix multiplication as a transformation (or function) of $\vec{x}$. 

Transformations encoded with matrices are a special type of function called a linear transformation, which by definition have the following properties:

$$
\begin{align} \\
\forall(A\in \mathbb{R}^{m\times  n}\text{ and  }
u,v\in \mathbb{R}^{n}), \\ \\


A(\vec{u}+\vec{v}=A\vec{u}+A\vec{v}) \\
A(c\vec{u})=cA\vec{u} \\
\end{align}
$$

## Linear transformations
The unit square is the region whose edges are given by $\begin{bmatrix}1 & 0 \\0 & 1\end{bmatrix}$. After we perform the transformation $A$, we take the vectors from that unit square to the new coordinates of A. For instance, given $A=\begin{bmatrix}2&1\\0&2\end{bmatrix}$, this takes the x coordinate of the unit square to $(2,0)$ and the y coordinates to $(1,2)$. Linear transformations keep any parallel grid lines as parallel. This transformation takes rectangles to parallelograms, as it sheers the  unit square. We can visualize this by scaling each basis vector by each column vector in A, which comes out as typical matrix multiplication. 

Determinants tell us about the change in area of shapes that are scaled through linear transformations. Because transformations are strictly linear, the change in volume of any shape in a space is the same for all shapes.

For instance, if we transform 
$AI$, the parallelogram takes the area for the $1\times 1$ square (A=1) to the area of a parallelogram (A=4). We can compute this by saying
$\text{ Area(A(S)) }=\left| \det(A) \right|\times \text{ Area(S) }$. We take the area from before applying a transformation and multiply by how much area is scaled to find the new area. 

Recall from before:
$$
\begin{align}
A=\begin{bmatrix}
a & b\\ c & d
\end{bmatrix} \\ \\

A^{-1}=\frac{1}{\boxed{(ad-bc)}}\begin{bmatrix}
d & -b \\ -c  &  a
\end{bmatrix} \\
\text{ Det(A) }=ad-bc
\end{align}
$$
if $Det(A) = 0$ then we knew that A is not invertible. This translates geometrically to the shape being taken to 0 area, i.e. compressing down dimensions. 

For a visual proof of $Det(A)$ in 2 dimensions: 

![[Pasted image 20240219112057.png]]
We take the entire area of the shape and subtract off any extra sides to find the area of the new shape. This gets the area of the $1\times 1$ square translated to a new shape as $ad-bc$. 

$$
\begin{align}
\text{ how much the area changes } &=\frac{\text{ final size }}{\text{ initial size }}\\
 \det(A) &= \frac{ad-bc}{1} 
\end{align}
$$
Please note that the determinant can be negative, which means that the x axis and y axis have "swapped places", meaning the +y axis is counterclockwise of +x instead of clockwise. This means that shapes have inverted.

What about the determinant in more dimensions?

If we take 
$$
A = \begin{bmatrix}
1 & 3 & 0 \\
0 & 1 & 2 \\
2 & 1 & 0
\end{bmatrix},
$$
$Det(A) = 10$. If we take a $1\times 1\times 1$ cube in $\mathbb{R}^{3}$ and apply A, the volume scales to be $10$. 

## Calculating determinants of $n\times n$ matrices

Theorem(cofactor expansion or Laplace expansion)
Let A be an $n\times n$ matrix with $n>2$.
$A_{i,j}$ is called the "cofactor". $A_{i,j}$ is the submatrix obtained by deleting the $i^{th}\text{ row and } j^{th}\text{ column }$.
Then 
$$
\begin{align}
det(A) &= a_{1,1}\det(A_{1,1}) - a_{1,2}\det(A_{1,2})+\dots+(-1^{n}a_{1,n}\det(A_{1,n})) \\
&= \sum_{j=1}^{n}(-1)^{i+j}a_{i,j}\det(A_{i,j}) \\
&\text{ which can also be written by expanding columns: } \\
&=\sum_{i=1}^{n}(-1)^{i+j}a_{i,j}\det(A_{i,j})
\end{align}
$$

For example,
$$
\begin{align}
\det A &= 1 \begin{vmatrix}
1 & 2 \\
1 & 0
\end{vmatrix}-3\begin{vmatrix}
0 & 2 \\
2 & 0
\end{vmatrix}+0\begin{vmatrix}
0 & 1 \\
2 & 1
\end{vmatrix} \\
&=1(1\times  0-2\times  1)-3(0\times  0-2\times  2)+0(0\times  1-1\times  2) \\
&=-2+10 \\
&= 10
\end{align}
$$
*note for the future:* it isn't a coincidence that this looks like a cross product.

We can calculate this more easily in some cases though.
The determinant of a *triangular matrix* is the product of its diagonal elements. 

![[Pasted image 20240219114336.png]]

Note by geometry:
$\det(AB)=\det(A) \times \det(B)$. Each transformation acts equally on all of space, its just scaling something. 

## Row operations and Properties of determinants

EROs $\equiv$ multiplication by elementary matrices.
For instance, row swap:
$$
\begin{align}
R_{1}\leftrightarrow R_{2},\, \, \, \, \, \, 
E_{1}=\begin{bmatrix}
0 & 1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1
\end{bmatrix} \\
\implies \det(E_{1})=-1
\end{align}
$$
for scalar multiple
$$
\begin{align}
R_{1} \leftarrow kR_{1},\, \, \, E_{2}=\begin{bmatrix}
k & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}\\
\det(E_{2})=k
\end{align}
$$
Which aligns with our geometric intuition - multiplying a dimension by a scalar multiplies the area by that scalar.

adding multiples of rows to another:
$$
\begin{align}
R_{1}\leftarrow R_{3}+kR_{2} \, \, \, E_{3}=\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & k & 1
\end{bmatrix} \\
\det(E_{3})=1
\end{align}
$$
This gives us a much nicer way to compute the determinant. We can just decompose a transformation A into a set of elementary multiplications. We just multiply each of the determinants of those elementary operations to find the final!  



