 Recall: 
	EROs are really just multiplication by an elementary matrix on which that ERO has been applied.
	Given a matrix
	$(A|I)$
	If A is invertible, after RREF A is the identity and I becomes $A^{-1}$ 
	**Only square matrices have inverses, as they have to go down to the identity**. If A doesn't map to the identity matrix, we don't have an inverse to undo A. 
	Theorem: Let $A$ be a square matrix. Any sequence of row operations that reduces $A$ to $I$ also reduced $I$ to $A^{-1}$.
	We can represent those row operations as matrix multiplication $E_{k}\dots E_{2}E_{1}$, where $E_{1}$ is the first ERO that is done. $E_{k}\dots E_{2}E_{1}A=I$, then $B=E_{k}\dots E_{2}E_{1}$, meaning $BA=I$, B is the inverse of A. 
	$A^{-1}=B=BI=E_{k}\dots E_{2}E_{1}$
	Practice: 
	For the excitable reader, compute the inverse of
	$$A=
	\begin{bmatrix}
	1 & -1 & 0 \\
	2 & 0 & 3 \\
	-1 & 4 & -5
	\end{bmatrix}
	$$
	Hint:
		We take the EROs that map A to the Identity, we represent each of those EROs as a matrix. To compute the inverse, we multiply those ERO matrixes by the identity matrix.

Lets begin by talking about logical claims that are equivalent. 
For example: 
"The integer n is even" and "the integer n can be written as $2k$ for some k". 
These are equivalent statements, as they are functionally the exact same. If one is true, the other is true in all cases. If one is false, the other is false in all cases. There are whole groups of equivalent statements, but all mean the same thing. 

If it is raining ($\mathbf{P}$) $\implies$ then it is cloudy $\mathbf{Q}$

The contrapositive:
If it is not cloudy ($\urcorner\mathbf{Q}$) $\implies$ it is not raining ($\urcorner\mathbf{P}$).

![[Pasted image 20240207110316.png|300]]
There are 5 statements, so $5 \choose 2$ statements. However, the 5 are all equivalent. We only need to prove a cycle through the network of implications, then walk to the conclusion. 


#subspace: A subspace of $\mathbb{R}^{n}$ is any collection of vectors in $\mathbb{R}^{^{n}}$ such that 
1) The zero vector $\vec{0}$ is in $S$
2) If $\vec{u},\vec{v}$ are in $S\text{ and } c,d$ are scalars, then $c\vec{u}+d\vec{v}$ is in S
In words, a subspace is a subset of $\mathbb{R}^{n}$ that contains $\vec{0}$ and is closed under scalar multiplication.


Subspaces of $\mathbb{R}^{2}:$ $\mathbb{R}^{2}$, lines through $\vec{0}$, $\vec{0}$
Subspaces of $\mathbb{R}^{3}:$ $\mathbb{R}^{3}$, planes through $\vec{0}$, $\vec{0}$. 
this generalizes to any $\mathbb{R}^{n}$ with n-dimensional objects passing through $\vec{0}$ or $\vec{0}$

Special subspaces:

$Col(A) \subseteq \mathbb{R}^{m}$: the set of linear combinations from the columns of a matrix
$Row(A) \subseteq \mathbb{R}^{n}$: the set of linear combinations from the rows of a matrix
$Null(A)\subseteq \mathbb{R}^{n}$: the null space is the set of vectors that would map to zero

To test if $\vec{w} \in Row(A)$, augment using ERO's (without swaps). 
if $w\in Row(A)$, then W is in  a linear combination of the rows of A, so we should be left with a row of 0's

**Every span of vectors is a subspace and every subspace can be written as a span of vectors.**

For example, take the matrix of vectors A B C D
$$
\begin{bmatrix}
1 & 2 & 1 & 2 \\
1 & 2 & 2 & 3 \\
1 & 2 & 3 & 4
\end{bmatrix}
$$
We have two vectors that are multiples of each other, so we can say
span(A,B,C,D) = Span(A,B)

We can verify this algebraically. If a vector $\vec{v}$ is in the span $\vec{v}_{1},\vec{v}_{2},\vec{v}_{3},\vec{v}_{4}$, then 

$$
\begin{align}
\vec{v}=c_{1}\vec{v}_{1}+c_{2}\vec{v}_{2}+c_{3}\vec{v}_{3}+c_{4}\vec{v}_{4} \\
\vec{c}_{1}\vec{v}_{1}+\vec{c}_{2}(2\vec{v}_{1})+c_{3}\vec{v}_{3}+c_{4}(\vec{v}_{1}+\vec{v}_{2}) \\
= (c_{1}+2c_{2}+c_{4})\vec{v}_{1}+(c_{3}+c_{4})\vec{v}_{3} \\
v \in \, an(\vec{v}_{1},\vec{v}_{2})
\end{align}
$$
def: a #basis for a subspace $H$ is a set of vectors $B=\{ \vec{v}_{1},\dots,\vec{v}_{k} \}$ such that:
1) span(B) = H
2) B is a linearly independent set, with no redundancies
So each vector $\vec{v}\in H$ can be written as a linear combination of basis vectors.

For example: $\mathbb{R}^{3}$ (as a subspace of itself) has a basis $$
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
$$
But we can also form a basis with others
$$
\begin{bmatrix}
1 & 1 & 1 \\
0 & 1 & 1 \\
0 & 0 & 1
\end{bmatrix}
$$
and still represent any vector as combinations of these basis.
We can see transformations differently when we represent them with different bases.

theorem: every $\vec{v} \in H$ can be expressed as a linear combination of given basis vectors *in only one way*. 
Proof: let $B = \{ \vec{v}_{1},\dots ,\vec{v}_{k} \}$ be a basis for H. 
Suppose we can write $\vec{w}$ in two ways for contradiction.
$\vec{w}=a_{1}\vec{v}_{1}+\dots+\vec{a}_{k}\vec{v}_{k}$
$\vec{w}=b_{1}\vec{v}_{1}+\dots+b_{k}\vec{v}_{k}$
We can subtract and say it has to be equal to $\vec{0}$
$\vec{w}-\vec{w}=\vec{0} = (a_{1}-b_{1})\vec{v}_{2}\dots (a_{k}-b_{k})\vec{v}_{k}$
Each $a_{k}-b_{k}$ has to be zero, as they B is linearly independent and $\vec{w}=\vec{w}$.
So, each $a_{i}=b_{i}\implies$ only one way to express $\vec{w}$.

If we know the basis $B$, then $\vec{w}$ can be specified with the coordinates with respect to $B$. 

note: a subspace can have many bases, but **Any 2 bases for H must have the same number of vectors.** We call this number the dimension of H, denoted $dim(h)$
Example: Basis for the xy plane $P$ in $\mathbb{R}^{3}$.
$$
\left\{  \begin{bmatrix}
1\\0\\0
\end{bmatrix}\begin{bmatrix}
0\\1\\0
\end{bmatrix}  \right\} 
$$ has dimension 2.

We call H a line if it has dim = 1, a plane dim = 2, etc.

We can use RREF to find bases for $Row(A)+Col(A)$

Example
$$
\begin{bmatrix}
A=1 & 2 & 3 & 4 & 5  \\
6 & 7 & 8 & 9 & 10 \\
11 & 12 & 13 & 14 & 15 \\
16 & 17 & 18 & 19 & 21
\end{bmatrix}
$$
has RREF
$$
\begin{bmatrix}
1 & 0 & -2 & -2 & 0 \\
0 & 1 & 2 & 3 & 0 \\
0 & \vec{0} & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 0
\end{bmatrix}
$$
just for a quick check, make sure you recognize this is the same as saying that

$$
\begin{align}
x_{1}-x_{3}-2x_{4}=0 \\
x_{2}+2x_{3}+3x_{4}=0 \\
x_{5}=0 \\
\implies \\
x_{1}=x_{5}+2x \\
x_{2}=-2x_{5}-3x_{4} \\
x_{3}=x_{3} \\
x_{4}=x_{4} \\
x_{5}=0
\end{align}
$$

let each row be denoted $\vec{r}_{n}$. $\{ \vec{r}_{1},\vec{r}_{2},\vec{r}_{3} \}=\text{ basis(Row}A\text{) }$
We can find the basis for Null(A) by solving $Ax=\vec{0}$ and putting it into parametric form (in terms of free variables)
**This will automatically be in terms of a basis!**
In this example
$$
\begin{bmatrix}
x_{1} \\
x_{2} \\
x_{3} \\
x_{4} \\
x_{5}
\end{bmatrix}=
x_{3}
\begin{bmatrix}
1 \\
-2 \\
1 \\
0 \\
0
\end{bmatrix}+x_{4}\begin{bmatrix}
2 \\
-3 \\
0 \\
1 \\
0
\end{bmatrix}
$$
. The two matrixes on the right form a basis for $Null(A)$. 

Def: $$
\begin{align}
dim(Nul l(A)) = \text{ nullity of A } = 2 \\
dim(Row(A)) = \text{ (row) rank of A } = 3\\
dim(col(A)) = \text{ (column) tank A} =3
\end{align}
$$
The sum of the nullity and the rank is the number of columns.
Theorem: For any $A_{m\times n}$, $row \,rank (A) = col \,rank (A)$
$rank(A)+nullity(A)=n$
Rank(A) = number of columns with leading 1's in RREF
nullity(A) = number of free variables
n = total number of columns

The dimension of a subspace is the number of vectors in the basis of that subspace. 

key ideas:
1) A basis is a set of linearly independent vectors that spans the space
2) the # of vectors in a basis tells  us the dimensions of the space
3) Using RREF, we can find the bases for Col(A)+row(A)
