Recall:
the characteristic polynomial of $A$ is $p(\lambda)= \det(A-\lambda I)$.
We are subtracting lambda down the diagonal and finding the determinant.
For example, let $A = \begin{bmatrix}7&2\\ -4&1\end{bmatrix}$. We can work this out to be
$$
\begin{align}
\det \left( \begin{bmatrix}
7-\lambda & 2 \\
-4 & 1-\lambda
\end{bmatrix} \right) \\
= (7-\lambda)(1-\lambda) - (2)(-4)
\end{align}
$$
Every polynomial of degree n has exactly n roots, counting multiplicity(a root that appears twice), even if they are in the complex plane.  This can be factored as
$$
p(\lambda) = (-1)^{n}(\lambda-\lambda_{1})^{n_{1}}(\lambda-\lambda_{2})^{n_{2}}\dots (\lambda-\lambda_{n})^{n_{n}}
$$
Lets see an example with the above matrix.

We set this polynomial equal to zero.
We evaluate as $\lambda^{2}-8 \lambda+15=0$
We got lucky here, as it becomes $(\lambda-5)(\lambda-3)$
which evaluates to $\lambda_{1}=5, \lambda_{2}=3$.

Eigenvalues are the only values such that $\det(a-\lambda I) = 0$. If we have a matrix whose determinant is 0, that matrix is not invertible. There are nontrivial solutions that solve the homogenous equation, aka there are nonzero vectors in the null space of that matrix. 

The Nullspace of $A-\lambda I$ is not only zero. The eigenspaces associated with the lambdas are the nullspaces. 

The #eigenspace of an eigenvalue $\lambda$ is defined by $E_{\lambda} - Null(A-\lambda I)$
The #geometric_multiplicity of $\lambda$ is the dimension of $E_{\lambda}$, ie. the number of linearly independent eigenvectors for $\lambda$. The geometric multiplicity is at least 1 and at most the algebraic multiplicity of $\lambda$. 

We can write out the matrix with our eigenvalues (remember to check - the determinant has to be zero, aka they are linearly dependent!)

$$
\begin{align}
\begin{bmatrix}
2 & 2 \\
-4 & -4
\end{bmatrix}\begin{bmatrix}
a \\ b
\end{bmatrix} = \vec{0} \\
E_{\lambda_{1}}= span\left\{ \begin{bmatrix}
-1 \\ 1
\end{bmatrix} \right\}
\end{align}
$$
which gets us the geometric multiplicity of $\lambda=5$ as 1.
We can do the same thing for $\lambda=3$
and get $E_{\lambda_{2}}= s pan\left\{ \begin{bmatrix}1 \\ -2\end{bmatrix} \right\}$
Note that $\vec{v}_{1} \text{ and }  \vec{v}_{2}$ are linearly independent. 

This gives us a basis for $\mathbb{R}^{2}$ as $b=\left\{ \vec{v}_{1},\vec{v}_{2} \right\}$. This is an intrinsic basis for the matrix, it describes it really well. 

The eigendata yields intrinsic information about the matrix A. 

Suppose $\mathbb{R}^{n}$ has basis $\left\{ \vec{v}_{1},\dots,\vec{v}_{n} \right\}$. 
$A\vec{v}=\lambda \vec{v}$ by definition. We can easily compute the transformation of A with these eigenvectors. Given *any* x $\in\,\mathbb{R}^{n}$, we can express X in terms of the eigenbasis. 

$$
\begin{align}
A\vec{x}=A(c_{1}\vec{v}_{1}+\dots+c_{n}\vec{v}_{n}) \\
= c_{1}A\vec{v}_{1}+\dots c_{n}A\vec{v}_{n} \\
A\vec{x}=c_{1}\lambda_{1}\vec{v}_{1}+\dots+c_{n}\lambda_{n}\vec{v}_{n}
\end{align}

$$

The action of $A$ onto $\vec{x}$ is the sum of scaling each eigen component by corresponding evaluated $\lambda_{i}$!

Unfortunately, not every matrix has a basis of eigenvectors. 

Consider the matrix
$$
A=\begin{bmatrix}
0 & -4 \\
4 & 0
\end{bmatrix}
$$
This rotates space counterclockwise by $\frac{\pi}{2}$ and scales everything by 4. We call the stretching "dilation". 

For this matrix, the eigenvalues are complex . There is no vector that stays in the span of itself. 

$$
\begin{bmatrix}
-\lambda & -4 \\
4 & -\lambda
\end{bmatrix}
$$
In this matrix, the characteristic polynomial is 
$$
\begin{align}
\lambda^{2}+16=0 \\
\lambda= \pm 4i
\end{align}
$$

## Similarity of two matrices
This is perhaps one of the most important concepts in linear algebra.

Two matrices A and B are similar if there exists an invertible matrix $P$ such that $P^{-1}AP=B$. 

For example: 
$\text{ let } A=\begin{bmatrix}7&2 \\ -4 & 1\end{bmatrix} \text{ and }  P = \begin{bmatrix}1&1 \\ -1 & -2\end{bmatrix}$
We find that $\det(P)=-1$. 
$P^{-1}=\frac{1}{-1}\begin{bmatrix}-2&-1 \\ 1 & 1\end{bmatrix}$
We can now compute:

$$
\begin{align}
P^{-1}AP=\begin{bmatrix}
2 & 1 \\
-1 & -1
\end{bmatrix}\begin{bmatrix}
7 & 2 \\
-4 & 1
\end{bmatrix}\begin{bmatrix}
1 & 1 \\
-1 & -2
\end{bmatrix} \\
= \begin{bmatrix}
2 & 1 \\
-1 & -1
\end{bmatrix}\begin{bmatrix}
5 & 3 \\
-5 & 6
\end{bmatrix} \\
= \begin{bmatrix}
5 & 0 \\
0 & 3
\end{bmatrix}
\end{align}
$$
so $A$ is similar to the matrix
$$
\begin{bmatrix}
5 & 0 \\
0 & 3
\end{bmatrix}
$$
What! Woah! This is a diagonal matrix, and the diagonals of this matrix are the eigenvalues that we just saw!

An $n\times n$ matrix is #diagonalizable if $A$ is similar to a diagonal matrix $D$, i.e. there exists an invertible matrix $P$ such that 
$$
P^{-1}AP=D=\begin{bmatrix}
d_{1} & 0 & \dots & 0 \\
0 & d_{2} & \dots & 0 \\
\vdots  &  & \ddots & \vdots \\
0 & \dots & 0 & d_{n}
\end{bmatrix}
$$

**Similar matrixes represent the same linear transformation in different basis.** 

Theorem: if $A$ and $B$ are similar, then they have the same determinant, rank, invertibility, characteristic polynomial, and eigenvalues.

$$
\begin{align}
B=P^{-1}AP \\
\det(B)&=\det(P^{-1}AP) \\
&= \det(P^{-1})\det(A)\det(P) \\
&= \det(A)
\end{align}
$$
The only difference is the eigenvectors because we have changed the basis. Any diagonal matrix can be reduced to the Identity matrix, and has basis that are just the regular axis. 

Suppose that $A$ is diagonalizable by matrix $P$. Then 
$$
\begin{align}
P^{-1}AP=D\\
AP=PD \\
A \begin{bmatrix}
\vec{P}_{1} & \vec{P}_{2} & \dots & \vec{P}_{n}
\end{bmatrix} = \begin{bmatrix}
\vec{P}_{1} & \vec{P}_{2} & \dots & \vec{P}_{n}
\end{bmatrix} \begin{bmatrix}
d_{1} & 0  & \\
0  & d_{2}  & \\
 &  & \ddots &  \\
 &  &  & d_{n}
\end{bmatrix}
\end{align}
$$
The columns of P have to be eigenvectors of the matrix A.
$$
\begin{align}
A\vec{P}_{1}=d_{1}\vec{P}_{1} \\
A\vec{P}_{n}=d_{n}\vec{P}_{n}
\end{align}
$$
This is to say that 
$$
\begin{align}
A\vec{P}_{1}=\lambda_{1}\vec{P}_{1} \\
A \vec{P}_{n}=\lambda_{n}\vec{P}_{n}
\end{align}
$$
The columns of $P$ are eigenvectors of $A$! 
The numbers along the diagonal $D$ are eigenvalues of $A$!.

**The key theorem here is that the following are equivalent statements**:
* The $n\times n$ matrix A is diagonalizable
* A has $n$ linearly independent eigenvectors
* The geometric multiplicity equals the algebraic multiplicity for each eigenvalue of $A$. 

Question:
is the following matrix diagonalizable?
$$
\begin{bmatrix}
3 & 3 \\
0 & 3
\end{bmatrix}
$$
Unfortunately no! But we can still use similarity that we will come to understand how the transformation $A$ behaves, even with complex eigenvalues or whatever.

Through similarity we will see the power of a change of basis and how looking from the right (intrrinsic) perspective we can transform our understanding of how $A$ behaves. 

Similarity plays an essential role in applications of linear algebra, including applications to differential equations, machine learning, AI, data science, etc. But you'll need to give them time to sink in. For now, just take it as a definition and learn to work with it. 



